{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Competition: Binary Prediction with a Rainfall Dataset\n",
    "\n",
    "This notebook contains the analysis and modeling approach for the Kaggle Playground Series - Season 5, Episode 3 competition. The goal is to predict rainfall for each day of the year, and the evaluation metric is the area under the ROC curve (AUC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Data manipulation libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Feature selection\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the datasets\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Training set shape: {train_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")\n",
    "print(f\"Sample submission shape: {sample_submission.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display the first few rows of each dataset\n",
    "print(\"Training data sample:\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Test data sample:\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Sample submission format:\")\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Check for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for missing values in the training set\n",
    "print(\"Missing values in training set:\")\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "# Check for missing values in the test set\n",
    "print(\"\\nMissing values in test set:\")\n",
    "print(test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Types and Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check data types\n",
    "print(\"Data types in training set:\")\n",
    "print(train_df.dtypes)\n",
    "\n",
    "# Summary statistics for training set\n",
    "print(\"\\nSummary statistics for training set:\")\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Target Variable Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check the distribution of the target variable\n",
    "target_counts = train_df['rainfall'].value_counts()\n",
    "print(\"Target variable distribution:\")\n",
    "print(target_counts)\n",
    "print(f\"Percentage of class 1 (rainfall): {target_counts[1] / len(train_df) * 100:.2f}%\")\n",
    "print(f\"Percentage of class 0 (no rainfall): {target_counts[0] / len(train_df) * 100:.2f}%\")\n",
    "\n",
    "# Visualize the target distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='rainfall', data=train_df)\n",
    "plt.title('Distribution of Rainfall (Target Variable)')\n",
    "plt.xlabel('Rainfall (0 = No, 1 = Yes)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the distribution of numerical features\n",
    "numerical_features = train_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "numerical_features.remove('id')  # Remove ID column\n",
    "numerical_features.remove('rainfall')  # Remove target variable\n",
    "\n",
    "# Create histograms for each numerical feature\n",
    "plt.figure(figsize=(20, 15))\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    plt.subplot(3, 4, i+1)\n",
    "    sns.histplot(train_df[feature], kde=True)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Feature Relationships with Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the relationship between features and the target variable\n",
    "plt.figure(figsize=(20, 15))\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    plt.subplot(3, 4, i+1)\n",
    "    sns.boxplot(x='rainfall', y=feature, data=train_df)\n",
    "    plt.title(f'{feature} vs Rainfall')\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate correlation matrix\n",
    "correlation_matrix = train_df.drop('id', axis=1).corr()\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sort features by correlation with the target variable\n",
    "target_correlations = correlation_matrix['rainfall'].drop('rainfall').sort_values(ascending=False)\n",
    "print(\"Features sorted by correlation with rainfall:\")\n",
    "print(target_correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Feature Pair Plots for Highly Correlated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Select top 5 features with highest correlation (absolute value) with the target\n",
    "top_features = target_correlations.abs().sort_values(ascending=False).head(5).index.tolist()\n",
    "top_features_df = train_df[top_features + ['rainfall']]\n",
    "\n",
    "# Create pair plots for these features\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.pairplot(top_features_df, hue='rainfall', diag_kind='kde')\n",
    "plt.suptitle('Pair Plots of Top 5 Features by Correlation with Rainfall', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Seasonal Patterns (if 'day' represents day of year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check if 'day' represents day of year (1-365)\n",
    "print(f\"Minimum day value: {train_df['day'].min()}\")\n",
    "print(f\"Maximum day value: {train_df['day'].max()}\")\n",
    "\n",
    "# If day represents day of year, analyze seasonal patterns\n",
    "if train_df['day'].max() <= 366:  # Accounting for leap years\n",
    "    # Group by day and calculate rainfall probability\n",
    "    daily_rainfall = train_df.groupby('day')['rainfall'].mean().reset_index()\n",
    "    daily_rainfall.columns = ['day', 'rainfall_probability']\n",
    "    \n",
    "    # Plot rainfall probability by day\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(daily_rainfall['day'], daily_rainfall['rainfall_probability'], '-o', markersize=3)\n",
    "    plt.title('Rainfall Probability by Day of Year')\n",
    "    plt.xlabel('Day of Year')\n",
    "    plt.ylabel('Probability of Rainfall')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a function for feature engineering to apply to both train and test sets\n",
    "def engineer_features(df):\n",
    "    # Create a copy of the dataframe to avoid modifying the original\n",
    "    df_engineered = df.copy()\n",
    "    \n",
    "    # Temperature range (difference between max and min temperature)\n",
    "    df_engineered['temp_range'] = df_engineered['maxtemp'] - df_engineered['mintemp']\n",
    "    \n",
    "    # Temperature deviation from average\n",
    "    df_engineered['temp_deviation'] = df_engineered['temparature'] - ((df_engineered['maxtemp'] + df_engineered['mintemp']) / 2)\n",
    "    \n",
    "    # Dew point depression (difference between temperature and dew point)\n",
    "    df_engineered['dewpoint_depression'] = df_engineered['temparature'] - df_engineered['dewpoint']\n",
    "    \n",
    "    # Create seasonal features using sine and cosine transformations (assuming day is day of year)\n",
    "    if df_engineered['day'].max() <= 366:  # Check if day represents day of year\n",
    "        df_engineered['day_sin'] = np.sin(2 * np.pi * df_engineered['day'] / 365)\n",
    "        df_engineered['day_cos'] = np.cos(2 * np.pi * df_engineered['day'] / 365)\n",
    "    \n",
    "    # Wind chill factor (simplified formula)\n",
    "    df_engineered['wind_chill'] = 13.12 + 0.6215 * df_engineered['temparature'] - 11.37 * (df_engineered['windspeed'] ** 0.16) + 0.3965 * df_engineered['temparature'] * (df_engineered['windspeed'] ** 0.16)\n",
    "    \n",
    "    # Interaction between humidity and temperature\n",
    "    df_engineered['humidity_temp'] = df_engineered['humidity'] * df_engineered['temparature']\n",
    "    \n",
    "    # Interaction between cloud cover and sunshine\n",
    "    df_engineered['cloud_sunshine'] = df_engineered['cloud'] * df_engineered['sunshine']\n",
    "    \n",
    "    # Wind direction as cyclic feature\n",
    "    df_engineered['wind_dir_sin'] = np.sin(2 * np.pi * df_engineered['winddirection'] / 360)\n",
    "    df_engineered['wind_dir_cos'] = np.cos(2 * np.pi * df_engineered['winddirection'] / 360)\n",
    "    \n",
    "    return df_engineered\n",
    "\n",
    "# Apply feature engineering\n",
    "train_engineered = engineer_features(train_df)\n",
    "test_engineered = engineer_features(test_df)\n",
    "\n",
    "# Display the new features\n",
    "new_features = [col for col in train_engineered.columns if col not in train_df.columns]\n",
    "print(f\"Newly created features: {new_features}\")\n",
    "train_engineered[new_features].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Separate features and target\n",
    "X = train_engineered.drop(['id', 'rainfall'], axis=1)\n",
    "y = train_engineered['rainfall']\n",
    "X_test = test_engineered.drop(['id'], axis=1)\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and validation data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrames to keep column names\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_val_scaled_df = pd.DataFrame(X_val_scaled, columns=X_val.columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "# Display the scaled data\n",
    "X_train_scaled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define a function to evaluate models\n",
    "def evaluate_model(model, X_train, y_train, X_val, y_val, model_name):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on validation set\n",
    "    y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Calculate AUC score\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "    cv_mean = np.mean(cv_scores)\n",
    "    cv_std = np.std(cv_scores)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Validation AUC: {auc:.4f}\")\n",
    "    print(f\"  Cross-validation AUC: {cv_mean:.4f} ± {cv_std:.4f}\")\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_val, y_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {model_name}')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return model, auc, cv_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define baseline models\n",
    "models = {\n",
    "    'Logistic Regression': Logistic<response clipped><NOTE>To save on context only part of this file has been shown to you. You should retry this tool after you have searched inside the file with `grep -n` in order to find the line numbers of what you are looking for.</NOTE>